---
title: "tharunte_Homework4"
output:
  pdf_document:
    latex_engine: xelatex
---

Question 1: 1.	Open link 1 of Links for Time Series. Run R commands pertinent to Ch.4-5. These commands are listed on pp.429-438.

# Chapter 4
```{R}
library(TSA)
```

## Moving Average

### MA1
```{R}
# Exhibit 4.2 on page 59.
data(ma1.2.s)
plot(ma1.2.s,ylab=expression(Y[t]),type='o')
```

```{R}
plot(y=ma1.2.s,x=zlag(ma1.2.s),ylab=expression(Y[t]),xlab=expression(Y[t-1]),type='p')
```

```{R}
plot(y=ma1.2.s,x=zlag(ma1.2.s,2),ylab=expression(Y[t]),xlab=expression(Y[t-2]),type='p')
```
### MA1
```{R}
data(ma1.1.s)
plot(ma1.1.s,ylab=expression(Y[t]),type='o')
```
```{R}
plot(y=ma1.1.s,x=zlag(ma1.1.s),ylab=expression(Y[t]),xlab=expression(Y[t-1]),type='p')
```
### MA2
Additional code from listed in ch 4 60 - 70
```{R}
data(ma2.s)
plot(ma2.s,ylab=expression(Y[t]),type='o')
plot(y=ma2.s,x=zlag(ma2.s),ylab=expression(Y[t]),xlab=expression(Y[t-1]),type='p')
plot(y=ma2.s,x=zlag(ma2.s,2),ylab=expression(Y[t]),xlab=expression(Y[t-2]),type='p')
plot(y=ma2.s,x=zlag(ma2.s,3),ylab=expression(Y[t]),xlab=expression(Y[t-3]),type='p')
```

## Autoregressive Processes

### AR1
```{R}
data(ar1.s)
plot(ar1.s,ylab=expression(Y[t]),type='o')
plot(y=ar1.s,x=zlag(ar1.s),ylab=expression(Y[t]),xlab=expression(Y[t-1]),type='p')
plot(y=ar1.s,x=zlag(ar1.s,2),ylab=expression(Y[t]),xlab=expression(Y[t-2]),type='p')
plot(y=ar1.s,x=zlag(ar1.s,3),ylab=expression(Y[t]),xlab=expression(Y[t-3]),type='p')
```
### AR2

```{R}
data(ar2.s)
plot(ar2.s,ylab=expression(Y[t]),type='o')
```

Continuation 
```{R}
set.seed(12345) 
y=arima.sim(model=list(ma=-c(-0.9)),n=100)
```
```{R}
list1=list(a=c(1,2,3),b=4,c=ts(c(5,6,7,8),start=c(2006,2),frequency=4))
list1
```
To retrieve an element of a list
```{R}
list1$c
```
```{R}
str(list1)
```

# Chapter 5
Exhibit 5.1 Monthly Price of Oil: January 1986–January 2006
```{R}
data(oil.price)
plot(oil.price, ylab='Price per Barrel',type='l')
```
Exhibit 5.3 An Explosive “AR(1)” Series
```{R}
data(explode.s)
plot(explode.s,ylab=expression(Y[t]),type='o')
```
Exhibit 5.4 The Difference Series of the Logs of the Oil Price Time 
```{R}
plot(diff(log(oil.price)),ylab='Change in Log(Price)',type='l')
```

Exhibit 5.5 Simulation of an IMA(2,2) Series with theta_1 = 1 and theta_2 = −0.6
```{R}
data(ima22.s)
plot(ima22.s,ylab='IMA(2,2) Simulation',type='o')
```
Exhibit 5.6 First Difference of the Simulated IMA(2,2) Series
```{R}
plot(diff(ima22.s),ylab='First Difference',type='o')
```
Exhibit 5.7 Second Difference of the Simulated IMA(2,2) Series
```{R}
plot(diff(ima22.s,difference=2),ylab='DifferencedTwice',type='o')
```

Exhibit 5.8 U.S. Electricity Generated by Month
```{R}
data(electricity); plot(electricity)
```
Exhibit 5.9 Time Series Plot of Logarithms of Electricity Values
```{R}
plot(log(electricity),ylab='Log(electricity)')
```
Exhibit 5.10 Difference of Logarithms for Electricity Time Series
```{R}
plot(diff(log(electricity)),ylab='Difference of Log(electricity)')
```
Exhibit 5.11 Log-likelihood versus Lambda
```{R}
BoxCox.ar(electricity)
```
This plots the log-likelihood function of the power parameter for the model that accounts for autocorrelation in the data.
```{R}
BoxCox.ar(electricity)
```

# Question 2
## 3.5: 
The data file wages contains monthly values of the average hourly wages (in dollars) for workers in the U.S. apparel and textile products industry for July 1981
through June 1987

(a) Display and interpret the time series plot for these data.
```{R}
data(wages)
plot(wages) 
```
Interpretation:  will see the trend of wages over time from 1981 to 1987. As the wages goes on increasing with time with some randomness.

(b) Use least squares to fit a linear time trend to this time series. Interpret the
regression output. Save the standardized residuals from the fit for further analysis.

```{R}
model1 = lm(wages~time(wages))
summary(model1)
```
Interpretation: The linear model fits a strong upward trend in average wages over time, with a significant positive slope of 0.2811, indicating wages increased by 28 cents each month. The high R-squared value (0.9728) shows the model explains 97% of the variation in wages, confirming an excellent fit. Both the intercept and slope are statistically significant with p-values near zero. Residuals are small, suggesting the model captures the trend effectively.


```{R}
plot(wages,type='o',ylab='y')
abline(model1)
```

```{R}
stan_mod1 <- rstandard(model1)
```
(c) Construct and interpret the time series plot of the standardized residuals from
part (b)
```{R}
plot(stan_mod1, type = "o", xlab="Time")
```
Interpretation: The time series plot of standardized residuals shows no clear patterns or trends, indicating a good fit for the linear model. The random scattering around zero confirms that the model effectively captures the data's trend.

(d) Use least squares to fit a quadratic time trend to the wages time series. Interpret the regression output. Save the standardized residuals from the fit for further analysis.

```{R}
quad_model <- lm(wages ~ time(wages) + time(wages)^2)
summary(quad_model)
```
Interpretation: The quadratic model shows a strong upward trend in wages, with a significant positive coefficient for time, indicating wages increase by about 28 cents per month. The high R-squared value (0.9728) suggests the model explains 97% of the variance in the data. Both coefficients are statistically significant, confirming the model's effectiveness in capturing wage trends.

```{R}
stan_quad = rstandard(quad_model)
```
(e) Construct and interpret the time series plot of the standardized residuals from
part (d)

```{R}
plot(stan_quad, type="o", xlab = 'Time')
```
Interpretation: 
The time series plot shows the average hourly wages in the U.S. apparel and textile industry from 1981 to 1987. Wages initially rise, peak around the middle, and then decline towards the end. This suggests variability with an overall downward trend later in the period. The fluctuations indicate both growth and decline phases.

## Question 3.6
The data file beersales contains monthly U.S. beer sales (in millions of barrels)
for the period January 1975 through December 1990.
(a) Display and interpret the plot the time series plot for these data

```{R}
data(beersales)
plot(beersales)
```
Clear seasonal trends. There is an initial positive trend from 1975 to around 1981 that then levels out.

(b) Now construct a time series plot that uses separate plotting symbols for the
various months. Does your interpretation change from that in part (a)?

```{R}
plot(beersales, type = "l", )
points(y=beersales,x=as.vector(time(beersales)),pch = as.vector(season(beersales)))
```
The time series plot with separate symbols for each month reveals a clear seasonal pattern in U.S. beer sales from 1975 to 1990. The sales peak consistently in the summer months, indicating higher demand during this period. This detailed view confirms the initial positive trend until 1981, followed by a leveling off, and highlights the regular seasonal fluctuations more clearly than the initial plot.

It is now evident that the peaks are in the warm months and the slump in the winter and fall months. December is a particular low point, while May, June, and July seem to be the high points.


(c) Use least squares to fit a seasonal-means trend to this time series. Interpret the
regression output. Save the standardized residuals from the fit for further analysis.

```{R}
beer_model1 <- lm(beersales ~ season(beersales))
summary(beer_model1)
```
```{R}
beer_stan <- rstandard(beer_model1)
```
(d) Construct and interpret the time series plot of the standardized residuals from
part (c). Be sure to use proper plotting symbols to check on seasonality in the
standardized residuals

```{R}
plot(beer_stan ~ time(beersales), type="l", xlab = 'Time')
points(y = beer_stan, x=as.vector(time(beersales)),pch = as.vector(season(beersales)), col = 1)
```

The plot of standardized residuals shows a clear upward trend, indicating that the model did not fully capture the increasing trend in beer sales. The residuals are not randomly scattered around zero, suggesting potential issues with the model fit. The presence of patterns implies that further adjustments might be needed to address these trends effectively.

(e) Use least squares to fit a seasonal-means plus quadratic time trend to the beer
sales time series. Interpret the regression output. Save the standardized residuals from the fit for further analysis

```{R}
beer_model2 <- lm(beersales ~ season(beersales) + time(beersales) +
                  I(time(beersales) ^ 2))
summary(beer_model2)
```
```{R}
beer_stan_quad = rstandard(beer_model2)
```
(f) Construct and interpret the time series plot of the standardized residuals from
part (e). Again use proper plotting symbols to check for any remaining seasonality in the residuals

```{R}
plot(beer_stan_quad  ~ time(beersales), type="l", xlab = 'Time')
points(y = beer_stan_quad, x=as.vector(time(beersales)),pch = as.vector(season(beersales)), col = 1)
```

Many of the values are still not being predicted successfully but at least we’re able to model the long term trend better.

## 3.12 
(Continuation of Exercise 3.6) Consider the time series in the data file beersales.
(a) Obtain the residuals from the least squares fit of the seasonal-means plus quadratic time trend model

```{R}
beer_quad_seasonal <- lm(beersales ~ time(beersales) + I(time(beersales)^2) + season(beersales))
beer_resid <- rstandard(beer_quad_seasonal)
```
(b) Perform a runs test on the standardized residuals and interpret the results.

```{R}
runs(beer_resid)
```

The runs test on the standardized residuals yields a p-value of 0.0127, indicating significant evidence against the null hypothesis of randomness. With only 79 observed runs compared to an expected 96.625, this suggests a non-random pattern in the residuals. The results imply that there may be underlying structures or seasonality not fully captured by the model, indicating potential issues with model fit.


(c) Calculate and interpret the sample autocorrelations for the standardized residuals.

```{R}
acf(beer_resid)

```

The sample autocorrelation function (ACF) plot for the standardized residuals shows several spikes outside the blue dashed lines, which represent the 95% confidence interval. This indicates significant autocorrelation at various lags. The presence of these spikes suggests that the residuals are not purely random and that there may be patterns or dependencies not captured by the model. This implies potential inadequacies in the model's ability to account for all underlying structures in the data.

(d) Investigate the normality of the standardized residuals (error terms). Consider
 histograms and normal probability plots. Interpret the plots.
```{R}
# install.packages("lattice")  # Run if the package isn't installed
library(lattice)
# install.packages("gridExtra")  # Run if the package isn't installed
library(gridExtra)
```
```{R}
figa <- 
  qqmath(beer_resid, xlab = "Theoretical quantities",
       asp = 1,
       ylab = "Studentized residuals",
       panel = function(x, ...) {
         panel.qqmathline(x, ...)
         panel.qqmath(x, ...)
       })

figb <- densityplot(beer_resid, xlab = "Studentized residuals")
gridExtra::grid.arrange(figa, figb, ncol = 2)
```

The QQ plot on the left shows that the studentized residuals closely follow the theoretical quantiles, indicating that they are approximately normally distributed. The density plot on the right shows a symmetric distribution centered around zero, further supporting normality. These plots suggest that the residuals from the beer sales series fit well with a normal distribution after applying linear, quadratic, and seasonal adjustments.